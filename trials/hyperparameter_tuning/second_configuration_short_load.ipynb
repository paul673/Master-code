{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791e3bd7",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Intermediate state 2)\n",
    "\n",
    "This notebook loads the model after 3000 training iterations based on the hyperparameters found with Optuna duing the first performed study. This was done since some of the metrics already were decent at this state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad8a76",
   "metadata": {},
   "source": [
    "## Run notebook from project root\n",
    "\n",
    "Adjust `path_to_top_dir = \"../../\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path_to_top_dir = \"../../\"\n",
    "sys.path.append(os.path.abspath(path_to_top_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.abspath(path_to_top_dir)\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c064c5e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import shutil\n",
    "from omegaconf import OmegaConf\n",
    "from collections import defaultdict\n",
    "from src.submodels.openpom.functions import fragance_propabilities_from_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a25d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import sqlite3\n",
    "from rdkit.Chem.Draw import MolsToGridImage, MolToImage\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from torch_scatter import scatter\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.submodels.gflownet.algo.config import TBVariant\n",
    "from src.submodels.gflownet.config import Config,init_empty\n",
    "import torch\n",
    "import gc\n",
    "from src.model.scent_trainer import ScentTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7e260",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc848a4",
   "metadata": {},
   "source": [
    "Specify save directory. A copy of this directory will be created for the new trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_base=\"hyperbest2long 3000 load\"\n",
    "save_index = 0\n",
    "if save_index != 0:\n",
    "    save_name = f\"{save_base}_{save_index}\"\n",
    "else:\n",
    "    save_name = save_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_log_dir = os.path.join(root_path, f\"logs/{save_name}/\")\n",
    "original_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb4379",
   "metadata": {},
   "source": [
    "## Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f62b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def increment_name(name):\n",
    "    # Regex to capture the base name and optional number suffix\n",
    "    match = re.match(r\"^(.*?)(?:_(\\d+))?$\", name)\n",
    "    if not match:\n",
    "        return name + \"_1\"\n",
    "    \n",
    "    base = match.group(1)\n",
    "    num = match.group(2)\n",
    "    \n",
    "    if num is None:\n",
    "        return f\"{base}_1\"\n",
    "    else:\n",
    "        return f\"{base}_{int(num) + 1}\"\n",
    "\n",
    "def init_proceed_training(prev_log_dir):\n",
    "    if prev_log_dir[-1] == \"/\":\n",
    "        new_log_dir = increment_name(prev_log_dir[:-1])\n",
    "    else:\n",
    "        new_log_dir = increment_name(prev_log_dir)\n",
    "    \n",
    "    if not os.path.exists(new_log_dir):\n",
    "        shutil.copytree(prev_log_dir,new_log_dir)\n",
    "        print(f\"Create new directory: {new_log_dir}\")\n",
    "    else:\n",
    "        raise FileExistsError(f\"Save directory {new_log_dir} already exists - Abort\")\n",
    "    return new_log_dir,prev_log_dir\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_trainer_from_checkpoint(prev_log_dir: str, trainer_class: type, start_at_step:int=None, num_training_steps:int=None, id:int=0):\n",
    "    \n",
    "    log_dir, prev_log_dir = init_proceed_training(prev_log_dir)\n",
    "    checkpoint_path = os.path.join(log_dir, \"model_final_save.pt\")\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    \n",
    "    # Restore the config\n",
    "    cfg = checkpoint[\"cfg\"]\n",
    "    cfg = OmegaConf.to_container(cfg, resolve=True)  \n",
    "    cfg_obj = OmegaConf.structured(Config)             \n",
    "    cfg_merged = OmegaConf.merge(cfg_obj, cfg)     \n",
    "    cfg = OmegaConf.to_object(cfg_merged)  \n",
    "\n",
    "    if log_dir != cfg.log_dir:\n",
    "        cfg.log_dir = log_dir\n",
    "\n",
    "    # Ensure the correct start step\n",
    "    if start_at_step:\n",
    "        cfg.start_at_step = start_at_step\n",
    "    else:\n",
    "        cfg.start_at_step = cfg.num_training_steps\n",
    "    print(f\"Start at step: {cfg.start_at_step}\")\n",
    "\n",
    "    # Ensure the correct start step\n",
    "    if num_training_steps:\n",
    "        cfg.num_training_steps = cfg.start_at_step+num_training_steps\n",
    "    else:\n",
    "        cfg.num_training_steps = cfg.start_at_step+cfg.num_training_steps\n",
    "\n",
    "\n",
    "    # If log_dir exists and shouldn't be overwritten, disable deletion\n",
    "    cfg.overwrite_existing_exp = True\n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = trainer_class(cfg)\n",
    "\n",
    "    # Load model weights\n",
    "    trainer.model.load_state_dict(checkpoint[\"models_state_dict\"][0])\n",
    "    if \"sampling_model_state_dict\" in checkpoint:\n",
    "        trainer.sampling_model.load_state_dict(checkpoint[\"sampling_model_state_dict\"][0])\n",
    "    else:\n",
    "        trainer.sampling_model.load_state_dict(checkpoint[\"models_state_dict\"][0])\n",
    "    trainer.model.eval()\n",
    "    trainer.sampling_model.eval()\n",
    "\n",
    "    trainer.model.to(trainer.device)\n",
    "    trainer.sampling_model.to(trainer.device)\n",
    "\n",
    "\n",
    "    return trainer, cfg, prev_log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run(trial):\n",
    "    state = {\n",
    "        \"models_state_dict\": [trial.model.state_dict()],\n",
    "        \"cfg\": trial.cfg,\n",
    "        \"env_ctx\": trial.model.env_ctx,\n",
    "        #\"model\": trial.model,\n",
    "    }\n",
    "    if trial.sampling_model is not trial.model:\n",
    "        state[\"sampling_model_state_dict\"] = [trial.sampling_model.state_dict()]\n",
    "    fn = pathlib.Path(trial.cfg.log_dir) / \"model_final_save.pt\"\n",
    "    with open(fn, \"wb\") as fd:\n",
    "        torch.save(\n",
    "            state,\n",
    "            fd,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b398da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_training(trainer,prev_log_dir):\n",
    "    trainer.run()\n",
    "    #with open(os.path.join(prev_log_dir, \"train.log\"), \"r\") as f_old, open(os.path.join(trainer.cfg.log_dir, \"train.log\"), \"a\") as f_new:\n",
    "    save_run(trainer)\n",
    "\n",
    "    gc.collect()\n",
    "    # Read contents\n",
    "    with open(os.path.join(prev_log_dir, \"train.log\"), \"r\", encoding=\"utf-8\") as f_old:\n",
    "        old_content = f_old.read()\n",
    "    with open(os.path.join(trainer.cfg.log_dir, \"train.log\"), \"r\", encoding=\"utf-8\") as f_new:\n",
    "        new_content = f_new.read()\n",
    "\n",
    "    #   Write new file with old content first\n",
    "    with open(os.path.join(trainer.cfg.log_dir, \"train.log\"), \"w\", encoding=\"utf-8\") as f_new:\n",
    "        f_new.write(old_content)\n",
    "        f_new.write(new_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c23ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_event_scalars(log_dirs):\n",
    "    merged_scalars = defaultdict(list)  # tag -> list of events (across all logs)\n",
    "\n",
    "    for log_dir in log_dirs:\n",
    "        ea = event_accumulator.EventAccumulator(log_dir)\n",
    "        ea.Reload()\n",
    "\n",
    "        # Get all scalar tags in this log directory\n",
    "        tags = ea.Tags().get('scalars', [])\n",
    "\n",
    "        for tag in tags:\n",
    "            merged_scalars[tag].extend(ea.Scalars(tag))\n",
    "\n",
    "    \n",
    "    #for tag, events in merged_scalars.items():\n",
    "        #merged_scalars[tag] = sorted(events, key=lambda e: e.step)\n",
    "    return merged_scalars, merged_scalars[tag][-1].step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6854396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_immediate_subdirs_with_string(log_dir,substring):\n",
    "    root_dir = os.path.dirname(log_dir.rstrip(\"/\"))\n",
    "    return [\n",
    "        os.path.join(root_dir, d)\n",
    "        for d in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, d)) and substring in d and \"validation\" not in d\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_df(log_dirs):\n",
    "    training_df = pd.DataFrame()\n",
    "    for l_dir in log_dirs:\n",
    "        if not os.path.exists(f\"{l_dir}/train/generated_objs_0.db\"):\n",
    "            print(f\"DB files does not exist for {l_dir}\")\n",
    "            continue\n",
    "        conn = sqlite3.connect(f\"{l_dir}/train/generated_objs_0.db\")\n",
    "        new_training_df = pd.read_sql_query(\"SELECT * FROM results\", conn)\n",
    "        training_df = pd.concat([training_df, new_training_df], ignore_index=True)\n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c286e94",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d07c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = find_immediate_subdirs_with_string(original_log_dir,save_base)\n",
    "log_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a15956",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, start_at_step = merge_event_scalars(log_dirs)\n",
    "start_at_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "trainer, cfg, prev_log_dir = load_trainer_from_checkpoint(\n",
    "    original_log_dir, \n",
    "    ScentTrainer, \n",
    "    start_at_step=start_at_step, \n",
    "    num_training_steps=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9345b54",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2631cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array_equal(old_test_data,trainer.test_data.idcs), np.array_equal(old_training_data,trainer.training_data.idcs)\n",
    "#old_test_data, old_training_data = trainer.test_data.idcs,trainer.training_data.idcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc5f4d",
   "metadata": {},
   "source": [
    "### Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467becd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = trainer.cfg.num_training_steps % max(len(trainer.training_data),1)\n",
    "epoch_idx = trainer.cfg.num_training_steps // max(len(trainer.training_data),1)\n",
    "batch_idx,epoch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ced67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_config = copy.deepcopy(cfg)\n",
    "validation_config.algo.valid_num_from_policy= 0\n",
    "validation_config.algo.valid_num_from_dataset= 1\n",
    "validation_config.log_dir = trainer.cfg.log_dir[:-1] + \"_validation\"\n",
    "validation_trainer = ScentTrainer(validation_config, print_config=False)\n",
    "validation_trainer.model = copy.deepcopy(trainer.model)\n",
    "validation_trainer.sampling_model = copy.deepcopy(trainer.model)\n",
    "validation_trainer.training_data = copy.deepcopy(trainer.training_data)\n",
    "validation_trainer.test_data = copy.deepcopy(trainer.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63243062",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_drawn = validation_trainer.cfg.algo.valid_num_from_policy\n",
    "n_from_dataset = validation_trainer.cfg.algo.valid_num_from_dataset\n",
    "n_drawn,n_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = validation_trainer.build_validation_data_loader()\n",
    "df = trainer.test_data.df.iloc[validation_trainer.test_data.idcs]\n",
    "#valid_df = pd.DataFrame()\n",
    "rows = []\n",
    "\n",
    "\n",
    "\n",
    "for batch, smiles in zip(valid_dl, df[\"nonStereoSMILES\"]):\n",
    "    info = validation_trainer.evaluate_batch(batch.to(validation_trainer.device), epoch_idx, batch_idx)\n",
    "    batch_idx_ = torch.arange(n_from_dataset, device=validation_trainer.device).repeat_interleave(batch.traj_lens)\n",
    "    cond_info = getattr(batch, \"cond_info\", None)\n",
    "    batched_cond_info = cond_info[batch_idx_] if cond_info is not None else None\n",
    "    fwd_cat, per_graph_out = validation_trainer.model(batch, batched_cond_info)\n",
    "    log_pF = fwd_cat.log_prob(batch.actions)\n",
    "    traj_log_p_F = scatter(log_pF, batch_idx_, dim=0, dim_size=n_from_dataset, reduce=\"sum\")\n",
    "    log_p_B = batch.log_p_B\n",
    "    traj_log_p_B = scatter(log_p_B, batch_idx_, dim=0, dim_size=n_from_dataset, reduce=\"sum\")\n",
    "    log_rewards = batch.log_rewards\n",
    "\n",
    "    row = {\n",
    "        'SMILES': smiles,\n",
    "        'traj_p_F': traj_log_p_F.exp().item(),  \n",
    "        'traj_p_B': traj_log_p_B.exp().item(),\n",
    "        'reward': log_rewards.exp().item()\n",
    "    }\n",
    "    row = { **row, **info}\n",
    "    rows.append(row)\n",
    "    \n",
    "    print(f\"{smiles:<40} traj_p_F:{traj_log_p_F.exp().item():.5f} traj_p_B:{traj_log_p_B.exp().item():.5f} reward:{log_rewards.exp().item():.2f}\" + \" \".join(f\"{k}:{v:.2f}\" for k, v in info.items()))\n",
    "    gc.collect()\n",
    "valid_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.to_csv(os.path.join(trainer.cfg.log_dir, \"example_validation.csv\"),index=None)\n",
    "valid_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309fefa",
   "metadata": {},
   "source": [
    "### Visualize validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8498789",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[[\"reward\",\"loss\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[[\"reward\",\"loss\"]].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[[\"reward\",\"loss\"]].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[[\"reward\",\"loss\"]].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66058f4d",
   "metadata": {},
   "source": [
    "Trajectory probability $p_F(\\tau)$ vs. reward R(x). High rewards should have a high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(valid_df, x=\"reward\", y=\"traj_p_F\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"R(x)\")\n",
    "plt.ylabel(r\"p$_F(\\tau)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85a210",
   "metadata": {},
   "source": [
    "Loss vs. reward R(x). High rewards should have a low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(valid_df, x=\"reward\", y=\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"R(x)\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76c792",
   "metadata": {},
   "source": [
    "Loss vs. reward R(x). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(valid_df, x=\"traj_lens\", y=\"traj_p_F\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Trajectory length\")\n",
    "plt.ylabel(r\"p$_F(\\tau)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0813fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(valid_df, x=\"traj_lens\", y=\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Trajectory length\")\n",
    "plt.ylabel(r\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f49a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=valid_df, x=\"traj_lens\")\n",
    "plt.xlabel(\"Trajectory length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553013a",
   "metadata": {},
   "source": [
    "## Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb678c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = find_immediate_subdirs_with_string(original_log_dir,save_base)\n",
    "log_dirs\n",
    "\n",
    "event_scalars, _ = merge_event_scalars(log_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a483f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ e.step for e in event_scalars['train_loss']] == [i+1 for i in range(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = find_immediate_subdirs_with_string(original_log_dir,save_base)\n",
    "log_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "event_scalars, _ = merge_event_scalars(log_dirs)\n",
    "x = [i+1 for i in range(len(event_scalars['train_loss']))]\n",
    "f, ax = plt.subplots(2, 2, figsize=(4*3,6))\n",
    "sns.lineplot(y=[ e.value for e in event_scalars['train_loss']],x=x, ax=ax[0,0])\n",
    "ax[0,0].set_yscale('log')\n",
    "ax[0,0].set_ylabel('TB loss')\n",
    "sns.lineplot(y=[ e.value for e in event_scalars['train_sampled_reward_avg']],x=x, ax=ax[0,1])\n",
    "ax[0,1].set_ylabel('Average reward')\n",
    "sns.lineplot(y=[ e.value for e in event_scalars['train_online_loss']],x=x, ax=ax[1,0])\n",
    "ax[1,0].set_yscale('log')\n",
    "ax[1,0].set_ylabel('Online loss')\n",
    "sns.lineplot(y=[ e.value for e in event_scalars['train_offline_loss']],x=x, ax=ax[1,1])\n",
    "ax[1,1].set_yscale('log')\n",
    "ax[1,1].set_ylabel('Offline loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891cd38d",
   "metadata": {},
   "source": [
    "## Generate Molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10054e4",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a98f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_molecules_from_model(gen_trainer, amont=25):\n",
    "    # Generate molecules\n",
    "    trajs = gen_trainer.algo.create_training_data_from_own_samples(gen_trainer.model, amont)\n",
    "    objs = [gen_trainer.ctx.graph_to_obj(i['result']) for i in trajs]\n",
    "    obj_props, is_valid = gen_trainer.task.compute_obj_properties(objs)\n",
    "    cond_info = gen_trainer.task.sample_conditional_information(len(obj_props), 1)\n",
    "    log_rewards = gen_trainer.task.cond_info_to_logreward(cond_info, obj_props)\n",
    "    \n",
    "    valid_idcs = torch.tensor([i for i in range(len(trajs)) if trajs[i].get(\"is_valid\", True)]).long()    \n",
    "\n",
    "    valid_idcs = valid_idcs[is_valid]\n",
    "    all_fr = torch.zeros((len(trajs), obj_props.shape[1]))\n",
    "    all_fr[valid_idcs] = obj_props\n",
    "    rewards = all_fr.flatten()\n",
    "\n",
    "    return objs, obj_props, is_valid, rewards, log_rewards, trajs, cond_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d216ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_top_molecules(n, df):\n",
    "    top_rewards = df.sort_values(by='r', ascending=False).head(n)\n",
    "    objs = [Chem.MolFromSmiles(smiles) for smiles in top_rewards[\"smi\"]]\n",
    "    rewards = top_rewards[\"r\"]\n",
    "    return objs, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e410c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_molecules(cfg, df):\n",
    "    n = cfg.algo.valid_num_from_policy\n",
    "    objs = [Chem.MolFromSmiles(smiles) for smiles in df[-n:][\"smi\"]]\n",
    "    rewards = df[-n:][\"r\"]\n",
    "    return objs, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af411ad",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18577a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs, obj_props, is_valid, rewards, log_rewards, trajs, cond_info = sample_molecules_from_model(trainer, amont=25)\n",
    "MolsToGridImage(objs, molsPerRow=5, subImgSize=(200, 120), legends=[f'reward: {r:.2f}' if r != 0 else \"Invalid\" for r in rewards  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea0a0d",
   "metadata": {},
   "source": [
    "## Analyze Training Molcules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06868c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = get_training_df(log_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs, rewards = get_n_top_molecules(25,training_df)\n",
    "MolsToGridImage(objs, molsPerRow=5, subImgSize=(200, 120), legends=[f'reward: {r:.2f}' for r in rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44237c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs, rewards = get_last_molecules(trainer.cfg, training_df) \n",
    "MolsToGridImage(objs, molsPerRow=5, subImgSize=(200, 120), legends=[f'reward: {r:.2f}' for r in rewards])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49cd6e2",
   "metadata": {},
   "source": [
    "## Prepare Validation files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18395ed1",
   "metadata": {},
   "source": [
    "Sample model 10000 times. Done iterativly due to memory restrictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d273ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564487b5",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.submodels.gflownet.algo.config import Backward, LossFN, NLoss,TBVariant\n",
    "from src.submodels.gflownet.envs.graph_building_env import ActionIndex, Graph, GraphAction, GraphActionCategorical,GraphActionType, GraphBuildingEnv, GraphBuildingEnvContext\n",
    "from src.submodels.gflownet.algo.trajectory_balance import shift_right\n",
    "def compute_batch_losses(\n",
    "        algo,\n",
    "        model,\n",
    "        batch,\n",
    "        num_bootstrap: int = 0,  # type: ignore[override]\n",
    "    ):\n",
    "        \"\"\"Compute the losses over trajectories contained in the batch\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: TrajectoryBalanceModel\n",
    "           A GNN taking in a batch of graphs as input as per constructed by `self.construct_batch`.\n",
    "           Must have a `logZ` attribute, itself a model, which predicts log of Z(cond_info)\n",
    "        batch: gd.Batch\n",
    "          batch of graphs inputs as per constructed by `self.construct_batch`\n",
    "        num_bootstrap: int\n",
    "          the number of trajectories for which the reward loss is computed. Ignored if 0.\"\"\"\n",
    "        dev = batch.x.device\n",
    "        # A single trajectory is comprised of many graphs\n",
    "        num_trajs = int(batch.traj_lens.shape[0])\n",
    "        log_rewards = batch.log_rewards\n",
    "        # Clip rewards\n",
    "        assert log_rewards.ndim == 1\n",
    "        clip_log_R = torch.maximum(\n",
    "            log_rewards, torch.tensor(algo.global_cfg.algo.illegal_action_logreward, device=dev)\n",
    "        ).float()\n",
    "        #print(\"num_trajs:\", num_trajs)\n",
    "        #print(\"log_rewards:\", log_rewards)\n",
    "        #print(\"clip_log_R:\", clip_log_R)\n",
    "        cond_info = getattr(batch, \"cond_info\", None)\n",
    "        invalid_mask = 1 - batch.is_valid\n",
    "        #print(\"invalid_mask:\", invalid_mask)\n",
    "        # This index says which trajectory each graph belongs to, so\n",
    "        # it will look like [0,0,0,0,1,1,1,2,...] if trajectory 0 is\n",
    "        # of length 4, trajectory 1 of length 3, and so on.\n",
    "        batch_idx = torch.arange(num_trajs, device=dev).repeat_interleave(batch.traj_lens)\n",
    "        # The position of the last graph of each trajectory\n",
    "        traj_cumlen = torch.cumsum(batch.traj_lens, 0)\n",
    "        final_graph_idx = traj_cumlen - 1\n",
    "        # The position of the first graph of each trajectory\n",
    "        first_graph_idx = shift_right(traj_cumlen)\n",
    "        final_graph_idx_1 = torch.maximum(final_graph_idx - 1, first_graph_idx)\n",
    "\n",
    "        fwd_cat: GraphActionCategorical  # The per-state cond_info\n",
    "        batched_cond_info = cond_info[batch_idx] if cond_info is not None else None\n",
    "        #print(batch_idx,first_graph_idx,final_graph_idx_1)\n",
    "    \n",
    "\n",
    "        # Forward pass of the model, returns a GraphActionCategorical representing the forward\n",
    "        # policy P_F, optionally a backward policy P_B, and per-graph outputs (e.g. F(s) in SubTB).\n",
    "        if algo.cfg.do_parameterize_p_b:\n",
    "            fwd_cat, bck_cat, per_graph_out = model(batch, batched_cond_info)\n",
    "        else:\n",
    "            if algo.model_is_autoregressive:\n",
    "                fwd_cat, per_graph_out = model(batch, cond_info, batched=True)\n",
    "            else:\n",
    "                fwd_cat, per_graph_out = model(batch, batched_cond_info)\n",
    "        \n",
    "        # Retreive the reward predictions for the full graphs,\n",
    "        # i.e. the final graph of each trajectory\n",
    "        log_reward_preds = per_graph_out[final_graph_idx, 0]\n",
    "        if algo.cfg.do_predict_n:\n",
    "            log_n_preds = per_graph_out[:, 1]\n",
    "            log_n_preds[first_graph_idx] = 0\n",
    "        else:\n",
    "            log_n_preds = None\n",
    "\n",
    "        # Compute trajectory balance objective\n",
    "        log_Z = model.logZ(cond_info)[:, 0]\n",
    "\n",
    "        # Compute the log prob of each action in the trajectory\n",
    "        if algo.cfg.do_correct_idempotent:\n",
    "            # If we want to correct for idempotent actions, we need to sum probabilities\n",
    "            # i.e. to compute P(s' | s) = sum_{a that lead to s'} P(a|s)\n",
    "            # here we compute the indices of the graph that each action corresponds to, ip_lens\n",
    "            # contains the number of idempotent actions for each transition, so we\n",
    "            # repeat_interleave as with batch_idx\n",
    "            ip_batch_idces = torch.arange(batch.ip_lens.shape[0], device=dev).repeat_interleave(batch.ip_lens)\n",
    "            # Indicate that the `batch` corresponding to each action is the above\n",
    "            ip_log_prob = fwd_cat.log_prob(batch.ip_actions, batch=ip_batch_idces)\n",
    "            # take the logsumexp (because we want to sum probabilities, not log probabilities)\n",
    "            # TODO: numerically stable version:\n",
    "            p = scatter(ip_log_prob.exp(), ip_batch_idces, dim=0, dim_size=batch_idx.shape[0], reduce=\"sum\")\n",
    "            # As a (reasonable) band-aid, ignore p < 1e-30, this will prevent underflows due to\n",
    "            # scatter(small number) = 0 on CUDA\n",
    "            log_p_F = p.clamp(1e-30).log()\n",
    "\n",
    "            if algo.cfg.do_parameterize_p_b:\n",
    "                # Now we repeat this but for the backward policy\n",
    "                bck_ip_batch_idces = torch.arange(batch.bck_ip_lens.shape[0], device=dev).repeat_interleave(\n",
    "                    batch.bck_ip_lens\n",
    "                )\n",
    "                bck_ip_log_prob = bck_cat.log_prob(batch.bck_ip_actions, batch=bck_ip_batch_idces)\n",
    "                bck_p = scatter(\n",
    "                    bck_ip_log_prob.exp(), bck_ip_batch_idces, dim=0, dim_size=batch_idx.shape[0], reduce=\"sum\"\n",
    "                )\n",
    "                log_p_B = bck_p.clamp(1e-30).log()\n",
    "        else:\n",
    "            # Else just naively take the logprob of the actions we took\n",
    "            log_p_F = fwd_cat.log_prob(batch.actions)\n",
    "\n",
    "            # Custom Code: Ensures that unprobabile actions dont get -inf as a Pf (Paul)\n",
    "            log_p_F = log_p_F.clamp(-69) # exp(-69) approx 1e-30\n",
    "\n",
    "            if algo.cfg.do_parameterize_p_b:\n",
    "                log_p_B = bck_cat.log_prob(batch.bck_actions)\n",
    "\n",
    "\n",
    "        if algo.cfg.do_parameterize_p_b:\n",
    "            # If we're modeling P_B then trajectories are padded with a virtual terminal state sF,\n",
    "            # zero-out the logP_F of those states\n",
    "            log_p_F[final_graph_idx] = 0\n",
    "            if algo.cfg.variant == TBVariant.SubTB1 or algo.cfg.variant == TBVariant.DB:\n",
    "                # Force the pad states' F(s) prediction to be R\n",
    "                per_graph_out[final_graph_idx, 0] = clip_log_R\n",
    "\n",
    "            # To get the correct P_B we need to shift all predictions by 1 state, and ignore the\n",
    "            # first P_B prediction of every trajectory.\n",
    "            # Our batch looks like this:\n",
    "            # [(s1, a1), (s2, a2), ..., (st, at), (sF, None),   (s1, a1), ...]\n",
    "            #                                                   ^ new trajectory begins\n",
    "            # For the P_B of s1, we need the output of the model at s2.\n",
    "\n",
    "            # We also have access to the is_sink attribute, which tells us when P_B must = 1, which\n",
    "            # we'll use to ignore the last padding state(s) of each trajectory. This by the same\n",
    "            # occasion masks out the first P_B of the \"next\" trajectory that we've shifted.\n",
    "            log_p_B = torch.roll(log_p_B, -1, 0) * (1 - batch.is_sink)\n",
    "        else:\n",
    "            log_p_B = batch.log_p_B\n",
    "        assert log_p_F.shape == log_p_B.shape\n",
    "\n",
    "        \n",
    "        if algo.cfg.n_loss == NLoss.TB:\n",
    "            log_traj_n = scatter(log_p_B, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\")\n",
    "            n_loss = algo._loss(log_traj_n + log_n_preds[final_graph_idx_1])\n",
    "        else:\n",
    "            n_loss = algo.n_loss(log_p_B, log_n_preds, batch.traj_lens)\n",
    "\n",
    "        if algo.ctx.has_n() and algo.cfg.do_predict_n:\n",
    "            analytical_maxent_backward = algo.analytical_maxent_backward(batch, first_graph_idx)\n",
    "            if algo.cfg.do_parameterize_p_b:\n",
    "                analytical_maxent_backward = torch.roll(analytical_maxent_backward, -1, 0) * (1 - batch.is_sink)\n",
    "        else:\n",
    "            analytical_maxent_backward = None\n",
    "\n",
    "        if algo.cfg.backward_policy in [Backward.GSQL, Backward.GSQLA]:\n",
    "            log_p_B = torch.zeros_like(log_p_B)\n",
    "            nzf = torch.maximum(first_graph_idx, final_graph_idx - 1)\n",
    "            if algo.cfg.backward_policy == Backward.GSQLA:\n",
    "                log_p_B[nzf] = -batch.log_n\n",
    "            else:\n",
    "                log_p_B[nzf] = -log_n_preds[\n",
    "                    nzf\n",
    "                ]  # this is due to the fact that n(s_0)/n(s1) * n(s1)/ n(s2) = n(s_0)/n(s2) = 1 / n(s)\n",
    "            # this is not final_graph_idx because we throw away the last thing\n",
    "        elif algo.cfg.backward_policy == Backward.MaxentA:\n",
    "            log_p_B = analytical_maxent_backward\n",
    "\n",
    "        if algo.cfg.do_parameterize_p_b:\n",
    "            # Life is pain, log_p_B is one unit too short for all trajs\n",
    "\n",
    "            log_p_B_unif = torch.zeros_like(log_p_B)\n",
    "            for i, (s, e) in enumerate(zip(first_graph_idx, traj_cumlen)):\n",
    "                log_p_B_unif[s : e - 1] = batch.log_p_B[s - i : e - 1 - i]\n",
    "\n",
    "            if algo.cfg.backward_policy == Backward.Uniform:\n",
    "                log_p_B = log_p_B_unif\n",
    "        else:\n",
    "            log_p_B_unif = log_p_B\n",
    "\n",
    "        if algo.cfg.backward_policy in [Backward.Maxent, Backward.GSQL]:\n",
    "            log_p_B = log_p_B.detach()\n",
    "        # This is the log probability of each trajectory\n",
    "        traj_log_p_F = scatter(log_p_F, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\")\n",
    "        traj_unif_log_p_B = scatter(log_p_B_unif, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\")\n",
    "        traj_log_p_B = scatter(log_p_B, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\")\n",
    "        #print(per_graph_out[:, 0])\n",
    "        if algo.cfg.variant == TBVariant.SubTB1:\n",
    "            # SubTB interprets the per_graph_out predictions to predict the state flow F(s)\n",
    "            if algo.cfg.cum_subtb:\n",
    "                #print(clip_log_R)\n",
    "                traj_losses = algo.subtb_cum(log_p_F, log_p_B, per_graph_out[:, 0], clip_log_R, batch.traj_lens)\n",
    "            else:\n",
    "                traj_losses = algo.subtb_loss_fast(log_p_F, log_p_B, per_graph_out[:, 0], clip_log_R, batch.traj_lens)\n",
    "\n",
    "            # The position of the first graph of each trajectory\n",
    "            first_graph_idx = torch.zeros_like(batch.traj_lens)\n",
    "            torch.cumsum(batch.traj_lens[:-1], 0, out=first_graph_idx[1:])\n",
    "            log_Z = per_graph_out[first_graph_idx, 0]\n",
    "        elif algo.cfg.variant == TBVariant.DB:\n",
    "            F_sn = per_graph_out[:, 0]\n",
    "            F_sm = per_graph_out[:, 0].roll(-1)\n",
    "            F_sm[final_graph_idx] = clip_log_R\n",
    "            transition_losses = algo._loss(F_sn + log_p_F - F_sm - log_p_B)\n",
    "            traj_losses = scatter(transition_losses, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\")\n",
    "            first_graph_idx = torch.zeros_like(batch.traj_lens)\n",
    "            torch.cumsum(batch.traj_lens[:-1], 0, out=first_graph_idx[1:])\n",
    "            log_Z = per_graph_out[first_graph_idx, 0]\n",
    "        else:\n",
    "            # Compute log numerator and denominator of the TB objective\n",
    "            numerator = log_Z + traj_log_p_F\n",
    "            denominator = clip_log_R + traj_log_p_B\n",
    "            if algo.mask_invalid_rewards:\n",
    "                # Instead of being rude to the model and giving a\n",
    "                # logreward of -100 what if we say, whatever you think the\n",
    "                # logprobablity of this trajetcory is it should be smaller\n",
    "                # (thus the `numerator - 1`). Why 1? Intuition?\n",
    "                denominator = denominator * (1 - invalid_mask) + invalid_mask * (numerator.detach() - 1)\n",
    "\n",
    "            if algo.cfg.epsilon is not None:\n",
    "                # Numerical stability epsilon\n",
    "                epsilon = torch.tensor([algo.cfg.epsilon], device=dev).float()\n",
    "                numerator = torch.logaddexp(numerator, epsilon)\n",
    "                denominator = torch.logaddexp(denominator, epsilon)\n",
    "            traj_losses = algo._loss(numerator - denominator, algo.tb_loss)\n",
    "\n",
    "\n",
    "        # Normalize losses by trajectory length\n",
    "        if algo.length_normalize_losses:\n",
    "            traj_losses = traj_losses / batch.traj_lens\n",
    "        if algo.reward_normalize_losses:\n",
    "            # multiply each loss by how important it is, using R as the importance factor\n",
    "            # factor = Rp.exp() / Rp.exp().sum()\n",
    "            factor = -clip_log_R.min() + clip_log_R + 1\n",
    "            factor = factor / factor.sum()\n",
    "            assert factor.shape == traj_losses.shape\n",
    "            # * num_trajs because we're doing a convex combination, and a .mean() later, which would\n",
    "            # undercount (by 2N) the contribution of each loss\n",
    "            traj_losses = factor * traj_losses * num_trajs\n",
    "\n",
    "        if algo.cfg.bootstrap_own_reward:\n",
    "            num_bootstrap = num_bootstrap or len(log_rewards)\n",
    "            reward_losses = algo._loss(log_rewards[:num_bootstrap] - log_reward_preds[:num_bootstrap], algo.reward_loss)\n",
    "\n",
    "            reward_loss = reward_losses.mean() * algo.cfg.reward_loss_multiplier\n",
    "        else:\n",
    "            reward_loss = 0\n",
    "\n",
    "        # TODO: Paul Clipp loss clipping based on variance \n",
    "        traj_losses_max = traj_losses.max()\n",
    "        traj_losses_min = traj_losses.min()\n",
    "        if algo.std_cut_losses:\n",
    "            # Cut the losses on a max value based on based on n times the standard deviation to reduce the impact of extreme outliers\n",
    "            traj_losses_mean = traj_losses.mean()\n",
    "            traj_losses_std = traj_losses.var().sqrt()\n",
    "            loss_cap = traj_losses_mean + traj_losses_std * algo.std_cut_scale\n",
    "            capped_traj_losses = torch.clamp(traj_losses, max=loss_cap) \n",
    "            traj_losses = capped_traj_losses\n",
    "\n",
    "\n",
    "        n_loss = n_loss.mean()\n",
    "        tb_loss = traj_losses.mean()\n",
    "        loss = tb_loss + reward_loss + algo.cfg.n_loss_multiplier * n_loss\n",
    "        info = {\n",
    "            \"offline_loss\": traj_losses[: batch.num_offline].mean() if batch.num_offline > 0 else 0,\n",
    "            \"online_loss\": traj_losses[batch.num_offline :].mean() if batch.num_online > 0 else 0,\n",
    "            \"reward_loss\": reward_loss,\n",
    "            \"invalid_trajectories\": invalid_mask.sum() / batch.num_online if batch.num_online > 0 else 0,\n",
    "            \"invalid_logprob\": (invalid_mask * traj_log_p_F).sum() / (invalid_mask.sum() + 1e-4),\n",
    "            \"invalid_losses\": (invalid_mask * traj_losses).sum() / (invalid_mask.sum() + 1e-4),\n",
    "            \"backward_vs_unif\": (traj_unif_log_p_B - traj_log_p_B).pow(2).mean(),\n",
    "            \"logZ\": log_Z.mean(),\n",
    "            \"loss\": loss.item(),\n",
    "            \"n_loss\": n_loss,\n",
    "            \"tb_loss\": tb_loss.item(),\n",
    "            \"batch_entropy\": -traj_log_p_F.mean(),\n",
    "            \"traj_lens\": batch.traj_lens.float().mean(),\n",
    "        }\n",
    "\n",
    "        if algo.std_cut_losses:\n",
    "            info[\"tb_loss_unclipped\"]= traj_losses_mean.item(), #Added by Paul \n",
    "            info[\"tb_loss_unclipped_std\"]= traj_losses_std.item(), #Added by Paul \n",
    "            info[\"tb_loss_max\"]= traj_losses_max.item(), #Added by Paul \n",
    "            info[\"tb_loss_min\"]= traj_losses_min.item(), #Added by Paul \n",
    "            \n",
    "        if algo.ctx.has_n() and algo.cfg.do_predict_n:\n",
    "            info[\"n_loss_pred\"] = scatter(\n",
    "                (log_n_preds - batch.log_ns) ** 2, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\"\n",
    "            ).mean()\n",
    "            info[\"n_final_loss\"] = torch.mean((log_n_preds[final_graph_idx] - batch.log_n) ** 2)\n",
    "            if algo.cfg.do_parameterize_p_b:\n",
    "                info[\"n_loss_tgsql\"] = torch.mean((-batch.log_n - traj_log_p_B) ** 2)\n",
    "                d = analytical_maxent_backward - log_p_B\n",
    "                d = d * d\n",
    "                d[final_graph_idx] = 0\n",
    "                info[\"n_loss_maxent\"] = scatter(d, batch_idx, dim=0, dim_size=num_trajs, reduce=\"sum\").mean()\n",
    "\n",
    "        return traj_log_p_B, traj_log_p_F, log_Z, clip_log_R, traj_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa86e783",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405418ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e39f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd\n",
    "OPENPOM_FILE_PATH = os.path.abspath(os.path.join(cwd, \"..\",\"..\",\"src\", \"data\", \"OpenPOM_probs.csv\"))\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    objs, obj_props, is_valid, rewards, log_rewards, trajs, cond_info  = sample_molecules_from_model(trainer, amont=100)\n",
    "    batch = trainer.algo.construct_batch(trajs, None, rewards.log())\n",
    "    traj_log_p_B, traj_log_p_F, log_Z, clip_log_R, traj_losses = compute_batch_losses(trainer.algo,trainer.model,batch)\n",
    "    barch_metrics_df = pd.DataFrame(dict(\n",
    "        traj_log_p_B=traj_log_p_B.detach().numpy(),\n",
    "        traj_log_p_F=traj_log_p_F.detach().numpy(),\n",
    "        log_Z=log_Z.detach().numpy(),\n",
    "        clip_log_R=clip_log_R.detach().numpy(), \n",
    "        subtb_losses=traj_losses.detach().numpy(), \n",
    "        tb_losses=(log_Z + traj_log_p_F -(clip_log_R + traj_log_p_B)).detach().numpy()\n",
    "        ))\n",
    "    smiles_lst = [Chem.MolToSmiles(m) for m in objs]\n",
    "    f_notes = []\n",
    "\n",
    "    cols= pd.read_csv(OPENPOM_FILE_PATH).keys()[1:]\n",
    "    for s in smiles_lst:\n",
    "        try:\n",
    "            f_notes.append(fragance_propabilities_from_smiles(s)[0])\n",
    "        except:\n",
    "            f_notes.append([None for i in range(138)])\n",
    "\n",
    "    probs_df = pd.DataFrame(f_notes, columns =cols) \n",
    "    objs_df = pd.DataFrame({\"SMILES\":[Chem.MolToSmiles(m) for m in objs], \"R(x)\": rewards, \"is_valid\": is_valid})\n",
    "    horizontal_concat_df = pd.concat([objs_df,barch_metrics_df, probs_df], axis=1) \n",
    "    results_df = pd.concat([results_df, horizontal_concat_df], axis=0)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9afd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"second_configuration_intermediate_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab71a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(results_df[['R(x)','vanilla']],diag_kws={\"bins\": 30})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
